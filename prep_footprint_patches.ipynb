{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "427ad357",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "from PIL import Image\n",
    "import yaml\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "50335e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Root of the YOLO-format AnimalClue footprint dataset\n",
    "YOLO_ROOT = Path(\n",
    "    r\"C:\\Users\\Kdbro\\OneDrive\\Desktop\\OSU\\Fall 2025\\Neural Networks\\Contrastive-and-Attribute-Aligned-Representation-Learning-for-Animal-Footprint\\dataset\\footprint_yolo\\species\"\n",
    ")\n",
    "\n",
    "PATCH_ROOT = Path(\n",
    "    r\"C:\\Users\\Kdbro\\OneDrive\\Desktop\\OSU\\Fall 2025\\Neural Networks\\Contrastive-and-Attribute-Aligned-Representation-Learning-for-Animal-Footprint\\dataset\\footprint_patches\"\n",
    ")\n",
    "\n",
    "# YOLO-style splits\n",
    "SPLITS = [\"train\", \"valid\", \"test\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ad779d63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inferred 117 classes.\n",
      "['species_000', 'species_001', 'species_002', 'species_003', 'species_004', 'species_005', 'species_006', 'species_007', 'species_008', 'species_009']\n"
     ]
    }
   ],
   "source": [
    "def infer_class_names_from_labels(yolo_root, splits=(\"train\", \"valid\", \"test\")):\n",
    "    \"\"\"\n",
    "    Scan YOLO label files and infer the set of class IDs.\n",
    "    Returns a list of generic class names 'species_000', 'species_001', ...\n",
    "    where index = class ID.\n",
    "    \"\"\"\n",
    "    yolo_root = Path(yolo_root)\n",
    "    class_ids = set()\n",
    "\n",
    "    for split in splits:\n",
    "        labels_dir = yolo_root / split / \"labels\"\n",
    "        if not labels_dir.exists():\n",
    "            print(f\"Warning: labels dir not found for split '{split}': {labels_dir}\")\n",
    "            continue\n",
    "\n",
    "        for label_path in labels_dir.glob(\"*.txt\"):\n",
    "            with label_path.open(\"r\") as f:\n",
    "                for line in f:\n",
    "                    line = line.strip()\n",
    "                    if not line:\n",
    "                        continue\n",
    "                    parts = line.split()\n",
    "                    try:\n",
    "                        cid = int(parts[0])\n",
    "                    except ValueError:\n",
    "                        continue\n",
    "                    class_ids.add(cid)\n",
    "\n",
    "    if not class_ids:\n",
    "        raise ValueError(\"No class IDs found in any label files.\")\n",
    "\n",
    "    num_classes = max(class_ids) + 1\n",
    "    missing = set(range(num_classes)) - class_ids\n",
    "    if missing:\n",
    "        print(\"Warning: some class IDs in [0, max] are not present in labels:\", sorted(missing))\n",
    "\n",
    "    # Create generic names\n",
    "    class_names = [f\"species_{i:03d}\" for i in range(num_classes)]\n",
    "    return class_names\n",
    "\n",
    "class_names = infer_class_names_from_labels(\n",
    "    YOLO_ROOT,        # e.g. ...\\dataset\\footprint_yolo\\species\n",
    "    splits=SPLITS     # [\"train\", \"valid\", \"test\"]\n",
    ")\n",
    "print(f\"Inferred {len(class_names)} classes.\")\n",
    "print(class_names[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4790f26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved class name mapping to: C:\\Users\\Kdbro\\OneDrive\\Desktop\\OSU\\Fall 2025\\Neural Networks\\Contrastive-and-Attribute-Aligned-Representation-Learning-for-Animal-Footprint\\dataset\\footprint_patches\\class_names.txt\n"
     ]
    }
   ],
   "source": [
    "PATCH_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "mapping_path = PATCH_ROOT / \"class_names.txt\"\n",
    "with mapping_path.open(\"w\") as f:\n",
    "    for idx, name in enumerate(class_names):\n",
    "        f.write(f\"{idx}\\t{name}\\n\")\n",
    "\n",
    "print(\"Saved class name mapping to:\", mapping_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b883b946",
   "metadata": {},
   "outputs": [],
   "source": [
    "def yolo_to_xyxy(bbox, img_width, img_height, margin_factor=0.1):\n",
    "    \"\"\"\n",
    "    Convert YOLO normalized bbox (cx, cy, w, h) to pixel coords (x_min, y_min, x_max, y_max).\n",
    "    margin_factor adds a bit of extra context around the footprint (e.g., 0.1 = 10%).\n",
    "\n",
    "    bbox: (cx, cy, w, h) all in [0,1]\n",
    "    \"\"\"\n",
    "    cx, cy, bw, bh = bbox\n",
    "\n",
    "    # Convert normalized center/size to pixel center/size\n",
    "    cx_pix = cx * img_width\n",
    "    cy_pix = cy * img_height\n",
    "    bw_pix = bw * img_width\n",
    "    bh_pix = bh * img_height\n",
    "\n",
    "    # Add margin\n",
    "    bw_pix *= (1.0 + margin_factor)\n",
    "    bh_pix *= (1.0 + margin_factor)\n",
    "\n",
    "    x_min = cx_pix - bw_pix / 2.0\n",
    "    x_max = cx_pix + bw_pix / 2.0\n",
    "    y_min = cy_pix - bh_pix / 2.0\n",
    "    y_max = cy_pix + bh_pix / 2.0\n",
    "\n",
    "    # Clip to image bounds\n",
    "    x_min = max(0, int(round(x_min)))\n",
    "    y_min = max(0, int(round(y_min)))\n",
    "    x_max = min(img_width - 1, int(round(x_max)))\n",
    "    y_max = min(img_height - 1, int(round(y_max)))\n",
    "\n",
    "    # Ensure valid box\n",
    "    if x_max <= x_min or y_max <= y_min:\n",
    "        return None\n",
    "\n",
    "    return x_min, y_min, x_max, y_max\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6858567e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_image_for_label(label_path, images_dir, img_exts=(\".jpg\", \".jpeg\", \".png\")):\n",
    "    \"\"\"\n",
    "    Given a label file path '.../xxx.txt', look for 'xxx.jpg' (or .jpeg/.png)\n",
    "    in images_dir.\n",
    "    \"\"\"\n",
    "    stem = label_path.stem\n",
    "    for ext in img_exts:\n",
    "        candidate = images_dir / f\"{stem}{ext}\"\n",
    "        if candidate.exists():\n",
    "            return candidate\n",
    "    return None\n",
    "\n",
    "\n",
    "def create_classification_patches(\n",
    "    yolo_root,\n",
    "    patch_root,\n",
    "    splits,\n",
    "    class_names,\n",
    "    margin_factor=0.1,\n",
    "    min_size=5,\n",
    "):\n",
    "    \"\"\"\n",
    "    Main preprocessing pipeline:\n",
    "      - Read YOLO labels\n",
    "      - Crop patches around footprints\n",
    "      - Save to PATCH_ROOT / split / class_name\n",
    "    \"\"\"\n",
    "    yolo_root = Path(yolo_root)\n",
    "    patch_root = Path(patch_root)\n",
    "    patch_root.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    for split in splits:\n",
    "        print(f\"\\n=== Processing split: {split} ===\")\n",
    "        # Now match your layout: species/train/images, species/train/labels, etc.\n",
    "        split_images_dir = yolo_root / split / \"images\"\n",
    "        split_labels_dir = yolo_root / split / \"labels\"\n",
    "\n",
    "        if not split_labels_dir.exists():\n",
    "            print(f\"Warning: labels dir for split '{split}' not found: {split_labels_dir}\")\n",
    "            continue\n",
    "\n",
    "        patch_count = 0\n",
    "        label_files = sorted(split_labels_dir.glob(\"*.txt\"))\n",
    "\n",
    "        for label_path in tqdm(label_files, desc=f\"{split} labels\"):\n",
    "            img_path = find_image_for_label(label_path, split_images_dir)\n",
    "            if img_path is None:\n",
    "                print(f\"Warning: no image found for label: {label_path}\")\n",
    "                continue\n",
    "\n",
    "            with Image.open(img_path) as img:\n",
    "                img = img.convert(\"RGB\")\n",
    "                w, h = img.size\n",
    "\n",
    "                with label_path.open(\"r\") as f:\n",
    "                    for idx_line, line in enumerate(f):\n",
    "                        line = line.strip()\n",
    "                        if not line:\n",
    "                            continue\n",
    "                        parts = line.split()\n",
    "                        if len(parts) != 5:\n",
    "                            print(f\"Bad label format in {label_path}: {line}\")\n",
    "                            continue\n",
    "\n",
    "                        class_id = int(parts[0])\n",
    "                        cx, cy, bw, bh = map(float, parts[1:])\n",
    "\n",
    "                        # Convert to pixel coords\n",
    "                        box = yolo_to_xyxy((cx, cy, bw, bh), w, h,\n",
    "                                           margin_factor=margin_factor)\n",
    "                        if box is None:\n",
    "                            continue\n",
    "                        x_min, y_min, x_max, y_max = box\n",
    "\n",
    "                        box_w = x_max - x_min\n",
    "                        box_h = y_max - y_min\n",
    "                        # Skip tiny patches (noise)\n",
    "                        if box_w < min_size or box_h < min_size:\n",
    "                            continue\n",
    "\n",
    "                        patch = img.crop((x_min, y_min, x_max, y_max))\n",
    "\n",
    "                        # Determine output dir: PATCH_ROOT/split/class_name\n",
    "                        class_name = class_names[class_id]\n",
    "                        out_dir = patch_root / split / class_name\n",
    "                        out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "                        out_name = f\"{img_path.stem}_{idx_line}.jpg\"\n",
    "                        out_path = out_dir / out_name\n",
    "                        patch.save(out_path)\n",
    "\n",
    "                        patch_count += 1\n",
    "\n",
    "        print(f\"Finished split '{split}'. Total patches saved: {patch_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8d17e691",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Processing split: train ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train labels: 100%|██████████| 5299/5299 [02:21<00:00, 37.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished split 'train'. Total patches saved: 12575\n",
      "\n",
      "=== Processing split: valid ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "valid labels: 100%|██████████| 757/757 [00:20<00:00, 36.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished split 'valid'. Total patches saved: 1882\n",
      "\n",
      "=== Processing split: test ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test labels: 100%|██████████| 1525/1525 [00:42<00:00, 36.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished split 'test'. Total patches saved: 3831\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "create_classification_patches(\n",
    "    yolo_root=YOLO_ROOT,\n",
    "    patch_root=PATCH_ROOT,\n",
    "    splits=SPLITS,\n",
    "    class_names=class_names,\n",
    "    margin_factor=0.1,\n",
    "    min_size=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "107e27d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FootprintPatchDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Classification-style dataset for cropped footprint patches.\n",
    "    Assumes directory structure:\n",
    "      root_dir/\n",
    "        class_name0/\n",
    "          *.jpg\n",
    "        class_name1/\n",
    "          *.jpg\n",
    "        ...\n",
    "    \"\"\"\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = Path(root_dir)\n",
    "        self.transform = transform\n",
    "\n",
    "        self.image_paths = []\n",
    "        self.labels = []\n",
    "        self.class_to_idx = {}\n",
    "        self.idx_to_class = []\n",
    "\n",
    "        # Walk subdirectories\n",
    "        for class_idx, class_name in enumerate(sorted(os.listdir(self.root_dir))):\n",
    "            class_path = self.root_dir / class_name\n",
    "            if not class_path.is_dir():\n",
    "                continue\n",
    "\n",
    "            self.class_to_idx[class_name] = class_idx\n",
    "            self.idx_to_class.append(class_name)\n",
    "\n",
    "            for fname in os.listdir(class_path):\n",
    "                if fname.lower().endswith((\".jpg\", \".jpeg\", \".png\")):\n",
    "                    self.image_paths.append(class_path / fname)\n",
    "                    self.labels.append(class_idx)\n",
    "\n",
    "        print(f\"Loaded {len(self.image_paths)} images \"\n",
    "              f\"from {self.root_dir}, {len(self.idx_to_class)} classes.\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        label = self.labels[idx]\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "317f0cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContrastiveTransform:\n",
    "    \"\"\"\n",
    "    Wraps a base transform and returns TWO augmented views.\n",
    "    \"\"\"\n",
    "    def __init__(self, base_transform):\n",
    "        self.base_transform = base_transform\n",
    "\n",
    "    def __call__(self, x):\n",
    "        xi = self.base_transform(x)\n",
    "        xj = self.base_transform(x)\n",
    "        return xi, xj\n",
    "\n",
    "image_size = 128  # must match your encoder\n",
    "\n",
    "base_transform = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(image_size, scale=(0.6, 1.0)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomApply([\n",
    "        transforms.ColorJitter(brightness=0.4, contrast=0.4,\n",
    "                               saturation=0.4, hue=0.1)\n",
    "    ], p=0.8),\n",
    "    transforms.RandomGrayscale(p=0.2),\n",
    "    transforms.GaussianBlur(kernel_size=9, sigma=(0.1, 2.0)),\n",
    "    transforms.ToTensor(),\n",
    "    # You can add normalization here if you like\n",
    "])\n",
    "\n",
    "contrastive_transform = ContrastiveTransform(base_transform)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1392d65d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 12575 images from C:\\Users\\Kdbro\\OneDrive\\Desktop\\OSU\\Fall 2025\\Neural Networks\\Contrastive-and-Attribute-Aligned-Representation-Learning-for-Animal-Footprint\\dataset\\footprint_patches\\train, 117 classes.\n",
      "Loaded 1882 images from C:\\Users\\Kdbro\\OneDrive\\Desktop\\OSU\\Fall 2025\\Neural Networks\\Contrastive-and-Attribute-Aligned-Representation-Learning-for-Animal-Footprint\\dataset\\footprint_patches\\valid, 117 classes.\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "\n",
    "train_dataset_contrastive = FootprintPatchDataset(\n",
    "    PATCH_ROOT / \"train\",\n",
    "    transform=contrastive_transform\n",
    ")\n",
    "\n",
    "train_loader_contrastive = DataLoader(\n",
    "    train_dataset_contrastive,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=4,\n",
    "    drop_last=True\n",
    ")\n",
    "\n",
    "# For supervised training / evaluation, you might want a simpler transform\n",
    "eval_transform = transforms.Compose([\n",
    "    transforms.Resize((image_size, image_size)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "val_dataset = FootprintPatchDataset(\n",
    "    PATCH_ROOT / \"valid\",\n",
    "    transform=eval_transform\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=4\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
