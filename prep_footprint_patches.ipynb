{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9eec1a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pyyaml tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "427ad357",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "from PIL import Image\n",
    "import yaml\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50335e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Root of the YOLO-format AnimalClue footprint dataset\n",
    "# The folder that contains 'images', 'labels', 'data.yaml'\n",
    "YOLO_ROOT = Path(\"/path/to/footprint_yolo\")\n",
    "\n",
    "# Where cropped patches are saved as a classification dataset\n",
    "PATCH_ROOT = Path(\"/path/to/footprint_patches\")\n",
    "\n",
    "# YOLO-style splits\n",
    "SPLITS = [\"train\", \"val\", \"test\"]\n",
    "\n",
    "YOLO_IMAGES_DIR = YOLO_ROOT / \"images\"\n",
    "YOLO_LABELS_DIR = YOLO_ROOT / \"labels\"\n",
    "DATA_YAML_PATH = YOLO_ROOT / \"data.yaml\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad779d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_class_names(yaml_path):\n",
    "    \"\"\"\n",
    "    Expects a YOLO data.yaml with a 'names' field, e.g.:\n",
    "      names: [dog, cat, ...]\n",
    "    Returns list: index -> class_name\n",
    "    \"\"\"\n",
    "    with open(yaml_path, \"r\") as f:\n",
    "        data = yaml.safe_load(f)\n",
    "    names = data.get(\"names\", None)\n",
    "    if isinstance(names, dict):\n",
    "        # Convert to list in index order\n",
    "        names = [names[k] for k in sorted(names.keys())]\n",
    "    return names\n",
    "\n",
    "class_names = load_class_names(DATA_YAML_PATH)\n",
    "print(f\"Loaded {len(class_names)} class names.\")\n",
    "print(class_names[:10])  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b883b946",
   "metadata": {},
   "outputs": [],
   "source": [
    "def yolo_to_xyxy(bbox, img_width, img_height, margin_factor=0.1):\n",
    "    \"\"\"\n",
    "    Convert YOLO normalized bbox (cx, cy, w, h) to pixel coords (x_min, y_min, x_max, y_max).\n",
    "    margin_factor adds a bit of extra context around the footprint (e.g., 0.1 = 10%).\n",
    "\n",
    "    bbox: (cx, cy, w, h) all in [0,1]\n",
    "    \"\"\"\n",
    "    cx, cy, bw, bh = bbox\n",
    "\n",
    "    # Convert normalized center/size to pixel center/size\n",
    "    cx_pix = cx * img_width\n",
    "    cy_pix = cy * img_height\n",
    "    bw_pix = bw * img_width\n",
    "    bh_pix = bh * img_height\n",
    "\n",
    "    # Add margin\n",
    "    bw_pix *= (1.0 + margin_factor)\n",
    "    bh_pix *= (1.0 + margin_factor)\n",
    "\n",
    "    x_min = cx_pix - bw_pix / 2.0\n",
    "    x_max = cx_pix + bw_pix / 2.0\n",
    "    y_min = cy_pix - bh_pix / 2.0\n",
    "    y_max = cy_pix + bh_pix / 2.0\n",
    "\n",
    "    # Clip to image bounds\n",
    "    x_min = max(0, int(round(x_min)))\n",
    "    y_min = max(0, int(round(y_min)))\n",
    "    x_max = min(img_width - 1, int(round(x_max)))\n",
    "    y_max = min(img_height - 1, int(round(y_max)))\n",
    "\n",
    "    # Ensure valid box\n",
    "    if x_max <= x_min or y_max <= y_min:\n",
    "        return None\n",
    "\n",
    "    return x_min, y_min, x_max, y_max\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6858567e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_image_for_label(label_path, images_dir, img_exts=(\".jpg\", \".jpeg\", \".png\")):\n",
    "    \"\"\"\n",
    "    Given a label file path '.../xxx.txt', look for 'xxx.jpg' (or .jpeg/.png)\n",
    "    in images_dir.\n",
    "    \"\"\"\n",
    "    stem = label_path.stem\n",
    "    for ext in img_exts:\n",
    "        candidate = images_dir / f\"{stem}{ext}\"\n",
    "        if candidate.exists():\n",
    "            return candidate\n",
    "    return None\n",
    "\n",
    "\n",
    "def create_classification_patches(\n",
    "    yolo_images_dir,\n",
    "    yolo_labels_dir,\n",
    "    patch_root,\n",
    "    splits,\n",
    "    class_names,\n",
    "    margin_factor=0.1,\n",
    "    min_size=5,\n",
    "):\n",
    "    \"\"\"\n",
    "    Main preprocessing pipeline:\n",
    "      - Read YOLO labels\n",
    "      - Crop patches around footprints\n",
    "      - Save to PATCH_ROOT / split / class_name\n",
    "    \"\"\"\n",
    "    patch_root = Path(patch_root)\n",
    "    patch_root.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    for split in splits:\n",
    "        print(f\"\\n=== Processing split: {split} ===\")\n",
    "        split_images_dir = yolo_images_dir / split\n",
    "        split_labels_dir = yolo_labels_dir / split\n",
    "\n",
    "        if not split_labels_dir.exists():\n",
    "            print(f\"Warning: labels dir for split '{split}' not found: {split_labels_dir}\")\n",
    "            continue\n",
    "\n",
    "        # Count patches \n",
    "        patch_count = 0\n",
    "\n",
    "        # All label files in this split\n",
    "        label_files = sorted(split_labels_dir.glob(\"*.txt\"))\n",
    "\n",
    "        for label_path in tqdm(label_files, desc=f\"{split} labels\"):\n",
    "            img_path = find_image_for_label(label_path, split_images_dir)\n",
    "            if img_path is None:\n",
    "                print(f\"Warning: no image found for label: {label_path}\")\n",
    "                continue\n",
    "\n",
    "            with Image.open(img_path) as img:\n",
    "                img = img.convert(\"RGB\")\n",
    "                w, h = img.size\n",
    "\n",
    "                with label_path.open(\"r\") as f:\n",
    "                    for idx_line, line in enumerate(f):\n",
    "                        line = line.strip()\n",
    "                        if not line:\n",
    "                            continue\n",
    "                        parts = line.split()\n",
    "                        if len(parts) != 5:\n",
    "                            print(f\"Bad label format in {label_path}: {line}\")\n",
    "                            continue\n",
    "\n",
    "                        class_id = int(parts[0])\n",
    "                        cx, cy, bw, bh = map(float, parts[1:])\n",
    "\n",
    "                        # Convert to pixel coords\n",
    "                        box = yolo_to_xyxy((cx, cy, bw, bh), w, h,\n",
    "                                           margin_factor=margin_factor)\n",
    "                        if box is None:\n",
    "                            continue\n",
    "                        x_min, y_min, x_max, y_max = box\n",
    "\n",
    "                        box_w = x_max - x_min\n",
    "                        box_h = y_max - y_min\n",
    "                        # Skip tiny patches (noise)\n",
    "                        if box_w < min_size or box_h < min_size:\n",
    "                            continue\n",
    "\n",
    "                        patch = img.crop((x_min, y_min, x_max, y_max))\n",
    "\n",
    "                        # Determine output dir: PATCH_ROOT/split/class_name\n",
    "                        class_name = class_names[class_id]\n",
    "                        out_dir = patch_root / split / class_name\n",
    "                        out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "                        out_name = f\"{img_path.stem}_{idx_line}.jpg\"\n",
    "                        out_path = out_dir / out_name\n",
    "                        patch.save(out_path)\n",
    "\n",
    "                        patch_count += 1\n",
    "\n",
    "        print(f\"Finished split '{split}'. Total patches saved: {patch_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc364411",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_classification_patches(\n",
    "    yolo_images_dir=YOLO_IMAGES_DIR,\n",
    "    yolo_labels_dir=YOLO_LABELS_DIR,\n",
    "    patch_root=PATCH_ROOT,\n",
    "    splits=SPLITS,\n",
    "    class_names=class_names,\n",
    "    margin_factor=0.1,   # tune this\n",
    "    min_size=10,         # skip super tiny patches\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107e27d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FootprintPatchDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Classification-style dataset for cropped footprint patches.\n",
    "    Assumes directory structure:\n",
    "      root_dir/\n",
    "        class_name0/\n",
    "          *.jpg\n",
    "        class_name1/\n",
    "          *.jpg\n",
    "        ...\n",
    "    \"\"\"\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = Path(root_dir)\n",
    "        self.transform = transform\n",
    "\n",
    "        self.image_paths = []\n",
    "        self.labels = []\n",
    "        self.class_to_idx = {}\n",
    "        self.idx_to_class = []\n",
    "\n",
    "        # Walk subdirectories\n",
    "        for class_idx, class_name in enumerate(sorted(os.listdir(self.root_dir))):\n",
    "            class_path = self.root_dir / class_name\n",
    "            if not class_path.is_dir():\n",
    "                continue\n",
    "\n",
    "            self.class_to_idx[class_name] = class_idx\n",
    "            self.idx_to_class.append(class_name)\n",
    "\n",
    "            for fname in os.listdir(class_path):\n",
    "                if fname.lower().endswith((\".jpg\", \".jpeg\", \".png\")):\n",
    "                    self.image_paths.append(class_path / fname)\n",
    "                    self.labels.append(class_idx)\n",
    "\n",
    "        print(f\"Loaded {len(self.image_paths)} images \"\n",
    "              f\"from {self.root_dir}, {len(self.idx_to_class)} classes.\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        label = self.labels[idx]\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "317f0cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContrastiveTransform:\n",
    "    \"\"\"\n",
    "    Wraps a base transform and returns TWO augmented views.\n",
    "    \"\"\"\n",
    "    def __init__(self, base_transform):\n",
    "        self.base_transform = base_transform\n",
    "\n",
    "    def __call__(self, x):\n",
    "        xi = self.base_transform(x)\n",
    "        xj = self.base_transform(x)\n",
    "        return xi, xj\n",
    "\n",
    "image_size = 128  # must match your encoder\n",
    "\n",
    "base_transform = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(image_size, scale=(0.6, 1.0)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomApply([\n",
    "        transforms.ColorJitter(brightness=0.4, contrast=0.4,\n",
    "                               saturation=0.4, hue=0.1)\n",
    "    ], p=0.8),\n",
    "    transforms.RandomGrayscale(p=0.2),\n",
    "    transforms.GaussianBlur(kernel_size=9, sigma=(0.1, 2.0)),\n",
    "    transforms.ToTensor(),\n",
    "    # You can add normalization here if you like\n",
    "])\n",
    "\n",
    "contrastive_transform = ContrastiveTransform(base_transform)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1392d65d",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "train_dataset_contrastive = FootprintPatchDataset(\n",
    "    PATCH_ROOT / \"train\",\n",
    "    transform=contrastive_transform\n",
    ")\n",
    "\n",
    "train_loader_contrastive = DataLoader(\n",
    "    train_dataset_contrastive,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=4,\n",
    "    drop_last=True\n",
    ")\n",
    "\n",
    "# For supervised training / evaluation, you might want a simpler transform\n",
    "eval_transform = transforms.Compose([\n",
    "    transforms.Resize((image_size, image_size)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "val_dataset = FootprintPatchDataset(\n",
    "    PATCH_ROOT / \"val\",\n",
    "    transform=eval_transform\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=4\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
