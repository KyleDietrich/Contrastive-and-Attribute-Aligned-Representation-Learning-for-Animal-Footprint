{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8f5d444a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "50621136",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FootprintPatchDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = Path(root_dir)\n",
    "        self.transform = transform\n",
    "\n",
    "        self.image_paths = []\n",
    "        self.labels = []\n",
    "        self.class_to_idx = {}\n",
    "        self.idx_to_class = []\n",
    "\n",
    "        for class_idx, class_name in enumerate(sorted(os.listdir(self.root_dir))):\n",
    "            class_path = self.root_dir / class_name\n",
    "            if not class_path.is_dir():\n",
    "                continue\n",
    "\n",
    "            self.class_to_idx[class_name] = class_idx\n",
    "            self.idx_to_class.append(class_name)\n",
    "\n",
    "            for fname in os.listdir(class_path):\n",
    "                if fname.lower().endswith((\".jpg\", \".jpeg\", \".png\")):\n",
    "                    self.image_paths.append(class_path / fname)\n",
    "                    self.labels.append(class_idx)\n",
    "\n",
    "        print(f\"Loaded {len(self.image_paths)} images \"\n",
    "              f\"from {self.root_dir}, {len(self.idx_to_class)} classes.\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        label = self.labels[idx]\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "26315f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FootprintEncoder(nn.Module):\n",
    "    def __init__(self, feature_dim=256):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, 3, 1, 1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(32, 64, 3, 1, 1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, 3, 1, 1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "        self.conv4 = nn.Sequential(\n",
    "            nn.Conv2d(128, 256, 3, 1, 1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.AdaptiveAvgPool2d((1, 1))\n",
    "        )\n",
    "        self.fc = nn.Linear(256, feature_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.conv4(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.fc(x)\n",
    "\n",
    "class BackboneEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Wrap torchvision backbones to output a feature vector h.\n",
    "    Supports: resnet50, vgg16, vit_b_16\n",
    "    \"\"\"\n",
    "    def __init__(self, name=\"resnet50\", pretrained=True):\n",
    "        super().__init__()\n",
    "        self.name = name\n",
    "\n",
    "        if name == \"resnet50\":\n",
    "            m = models.resnet50(weights=models.ResNet50_Weights.DEFAULT if pretrained else None)\n",
    "            self.backbone = nn.Sequential(*list(m.children())[:-1])  # remove fc\n",
    "            self.out_dim = 2048\n",
    "\n",
    "        elif name == \"vgg16\":\n",
    "            m = models.vgg16(weights=models.VGG16_Weights.DEFAULT if pretrained else None)\n",
    "            self.backbone = m.features\n",
    "            self.pool = nn.AdaptiveAvgPool2d((7, 7))\n",
    "            # VGG classifier input is 512*7*7\n",
    "            self.out_dim = 512 * 7 * 7\n",
    "\n",
    "        elif name == \"vit_b_16\":\n",
    "            m = models.vit_b_16(weights=models.ViT_B_16_Weights.DEFAULT if pretrained else None)\n",
    "            # remove classification head\n",
    "            m.heads = nn.Identity()\n",
    "            self.backbone = m\n",
    "            self.out_dim = 768\n",
    "\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown backbone name: {name}\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.name == \"resnet50\":\n",
    "            h = self.backbone(x)           # (B, 2048, 1, 1)\n",
    "            h = h.flatten(1)               # (B, 2048)\n",
    "            return h\n",
    "\n",
    "        elif self.name == \"vgg16\":\n",
    "            h = self.backbone(x)           # (B, 512, H, W)\n",
    "            h = self.pool(h)               # (B, 512, 7, 7)\n",
    "            h = h.flatten(1)               # (B, 25088)\n",
    "            return h\n",
    "\n",
    "        elif self.name == \"vit_b_16\":\n",
    "            h = self.backbone(x)           # (B, 768)\n",
    "            return h\n",
    "\n",
    "# Projector & Text encoder\n",
    "\n",
    "class ImageProjector(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim=128):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(in_dim, in_dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(in_dim, out_dim)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class TextTokenEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple text token encoder using an embedding layer.\n",
    "    take class ID as text token input , no species name\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes, out_dim=128):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(num_classes, out_dim)\n",
    "\n",
    "    def forward(self, labels):\n",
    "        return self.embedding(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e1a25fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clip_loss(image_z, text_z, temperature=0.07):\n",
    "    image_z = F.normalize(image_z, dim=1)\n",
    "    text_z = F.normalize(text_z, dim=1)\n",
    "\n",
    "    logits = (image_z @ text_z.t()) / temperature\n",
    "    targets = torch.arange(image_z.size(0), device=image_z.device)\n",
    "\n",
    "    loss_i2t = F.cross_entropy(logits, targets)\n",
    "    loss_t2i = F.cross_entropy(logits.t(), targets)\n",
    "    return 0.5 * (loss_i2t + loss_t2i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "638288da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_contrastive_encoder(backbone, path, device):\n",
    "    enc = BackboneEncoder(name=backbone, pretrained=False).to(device)\n",
    "    state_dict = torch.load(path, map_location=device)\n",
    "    enc.load_state_dict(state_dict)\n",
    "    enc.eval()\n",
    "    for p in enc.parameters():\n",
    "        p.requires_grad = False\n",
    "    print(f\"Loaded contrastive encoder : {backbone}\")\n",
    "    return enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4fdc789f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_rq2_alignment(\n",
    "    encoder, img_proj, txt_enc,\n",
    "    loader, device,\n",
    "    epochs=20, lr=1e-3,\n",
    "    temperature=0.07,\n",
    "    freeze_encoder=True\n",
    "):\n",
    "    encoder.to(device)\n",
    "    img_proj.to(device)\n",
    "    txt_enc.to(device)\n",
    "\n",
    "    if freeze_encoder:\n",
    "        encoder.eval()\n",
    "        for p in encoder.parameters():\n",
    "            p.requires_grad = False\n",
    "        params = list(img_proj.parameters()) + list(txt_enc.parameters())\n",
    "    else:\n",
    "        encoder.train()\n",
    "        for p in encoder.parameters():\n",
    "            p.requires_grad = True\n",
    "        params = list(encoder.parameters()) + list(img_proj.parameters()) + list(txt_enc.parameters())\n",
    "\n",
    "    opt = torch.optim.Adam(params, lr=lr, weight_decay=1e-4)\n",
    "\n",
    "    for ep in range(epochs):\n",
    "        total = 0.0\n",
    "        for imgs, labels in loader:\n",
    "            imgs = imgs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            feats = encoder(imgs)          # (B, output)\n",
    "            image_z = img_proj(feats)      # (B, 128)\n",
    "            text_z = txt_enc(labels)       # (B, 128)\n",
    "\n",
    "            loss = clip_loss(image_z, text_z, temperature)\n",
    "\n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "            total += loss.item()\n",
    "\n",
    "        print(f\"[RQ2 Align] epoch {ep+1}/{epochs}, loss={total/len(loader):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ef6ccc82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " RQ2 : small CNN encoder \n",
      "Loaded 12575 images from /users/PAS2985/tingle9/dataset/footprint_patches/train, 117 classes.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RQ2 Align] epoch 1/50, loss=3.6972\n",
      "[RQ2 Align] epoch 2/50, loss=3.3970\n",
      "[RQ2 Align] epoch 3/50, loss=3.2662\n",
      "[RQ2 Align] epoch 4/50, loss=3.1771\n",
      "[RQ2 Align] epoch 5/50, loss=3.1082\n",
      "[RQ2 Align] epoch 6/50, loss=3.0436\n",
      "[RQ2 Align] epoch 7/50, loss=2.9998\n",
      "[RQ2 Align] epoch 8/50, loss=2.9606\n",
      "[RQ2 Align] epoch 9/50, loss=2.9203\n",
      "[RQ2 Align] epoch 10/50, loss=2.8823\n",
      "[RQ2 Align] epoch 11/50, loss=2.8459\n",
      "[RQ2 Align] epoch 12/50, loss=2.8180\n",
      "[RQ2 Align] epoch 13/50, loss=2.7963\n",
      "[RQ2 Align] epoch 14/50, loss=2.7623\n",
      "[RQ2 Align] epoch 15/50, loss=2.7426\n",
      "[RQ2 Align] epoch 16/50, loss=2.7140\n",
      "[RQ2 Align] epoch 17/50, loss=2.6981\n",
      "[RQ2 Align] epoch 18/50, loss=2.6710\n",
      "[RQ2 Align] epoch 19/50, loss=2.6549\n",
      "[RQ2 Align] epoch 20/50, loss=2.6441\n",
      "[RQ2 Align] epoch 21/50, loss=2.6268\n",
      "[RQ2 Align] epoch 22/50, loss=2.6039\n",
      "[RQ2 Align] epoch 23/50, loss=2.5867\n",
      "[RQ2 Align] epoch 24/50, loss=2.5720\n",
      "[RQ2 Align] epoch 25/50, loss=2.5532\n",
      "[RQ2 Align] epoch 26/50, loss=2.5501\n",
      "[RQ2 Align] epoch 27/50, loss=2.5299\n",
      "[RQ2 Align] epoch 28/50, loss=2.5144\n",
      "[RQ2 Align] epoch 29/50, loss=2.5096\n",
      "[RQ2 Align] epoch 30/50, loss=2.5007\n",
      "[RQ2 Align] epoch 31/50, loss=2.4785\n",
      "[RQ2 Align] epoch 32/50, loss=2.4707\n",
      "[RQ2 Align] epoch 33/50, loss=2.4532\n",
      "[RQ2 Align] epoch 34/50, loss=2.4560\n",
      "[RQ2 Align] epoch 35/50, loss=2.4496\n",
      "[RQ2 Align] epoch 36/50, loss=2.4393\n",
      "[RQ2 Align] epoch 37/50, loss=2.4363\n",
      "[RQ2 Align] epoch 38/50, loss=2.4214\n",
      "[RQ2 Align] epoch 39/50, loss=2.4105\n",
      "[RQ2 Align] epoch 40/50, loss=2.4135\n",
      "[RQ2 Align] epoch 41/50, loss=2.3902\n",
      "[RQ2 Align] epoch 42/50, loss=2.4043\n",
      "[RQ2 Align] epoch 43/50, loss=2.3831\n",
      "[RQ2 Align] epoch 44/50, loss=2.3760\n",
      "[RQ2 Align] epoch 45/50, loss=2.3654\n",
      "[RQ2 Align] epoch 46/50, loss=2.3640\n",
      "[RQ2 Align] epoch 47/50, loss=2.3578\n",
      "[RQ2 Align] epoch 48/50, loss=2.3496\n",
      "[RQ2 Align] epoch 49/50, loss=2.3393\n",
      "[RQ2 Align] epoch 50/50, loss=2.3367\n",
      "saved RQ2 CNN checkpoints. \n",
      "\n",
      " RQ2: resnet50 encoder \n",
      "Loaded 12575 images from /users/PAS2985/tingle9/dataset/footprint_patches/train, 117 classes.\n",
      "Loaded contrastive encoder : resnet50\n",
      "[RQ2 Align] epoch 1/50, loss=3.3739\n",
      "[RQ2 Align] epoch 2/50, loss=3.0059\n",
      "[RQ2 Align] epoch 3/50, loss=2.8441\n",
      "[RQ2 Align] epoch 4/50, loss=2.7158\n",
      "[RQ2 Align] epoch 5/50, loss=2.6065\n",
      "[RQ2 Align] epoch 6/50, loss=2.5009\n",
      "[RQ2 Align] epoch 7/50, loss=2.4070\n",
      "[RQ2 Align] epoch 8/50, loss=2.3170\n",
      "[RQ2 Align] epoch 9/50, loss=2.2260\n",
      "[RQ2 Align] epoch 10/50, loss=2.1423\n",
      "[RQ2 Align] epoch 11/50, loss=2.0492\n",
      "[RQ2 Align] epoch 12/50, loss=1.9722\n",
      "[RQ2 Align] epoch 13/50, loss=1.9018\n",
      "[RQ2 Align] epoch 14/50, loss=1.8346\n",
      "[RQ2 Align] epoch 15/50, loss=1.7720\n",
      "[RQ2 Align] epoch 16/50, loss=1.7108\n",
      "[RQ2 Align] epoch 17/50, loss=1.6527\n",
      "[RQ2 Align] epoch 18/50, loss=1.5953\n",
      "[RQ2 Align] epoch 19/50, loss=1.5501\n",
      "[RQ2 Align] epoch 20/50, loss=1.4871\n",
      "[RQ2 Align] epoch 21/50, loss=1.4679\n",
      "[RQ2 Align] epoch 22/50, loss=1.4266\n",
      "[RQ2 Align] epoch 23/50, loss=1.3887\n",
      "[RQ2 Align] epoch 24/50, loss=1.3614\n",
      "[RQ2 Align] epoch 25/50, loss=1.3277\n",
      "[RQ2 Align] epoch 26/50, loss=1.3042\n",
      "[RQ2 Align] epoch 27/50, loss=1.2820\n",
      "[RQ2 Align] epoch 28/50, loss=1.2560\n",
      "[RQ2 Align] epoch 29/50, loss=1.2358\n",
      "[RQ2 Align] epoch 30/50, loss=1.2206\n",
      "[RQ2 Align] epoch 31/50, loss=1.2007\n",
      "[RQ2 Align] epoch 32/50, loss=1.1747\n",
      "[RQ2 Align] epoch 33/50, loss=1.1595\n",
      "[RQ2 Align] epoch 34/50, loss=1.1496\n",
      "[RQ2 Align] epoch 35/50, loss=1.1379\n",
      "[RQ2 Align] epoch 36/50, loss=1.1187\n",
      "[RQ2 Align] epoch 37/50, loss=1.1122\n",
      "[RQ2 Align] epoch 38/50, loss=1.1038\n",
      "[RQ2 Align] epoch 39/50, loss=1.0819\n",
      "[RQ2 Align] epoch 40/50, loss=1.0882\n",
      "[RQ2 Align] epoch 41/50, loss=1.0744\n",
      "[RQ2 Align] epoch 42/50, loss=1.0684\n",
      "[RQ2 Align] epoch 43/50, loss=1.0404\n",
      "[RQ2 Align] epoch 44/50, loss=1.0596\n",
      "[RQ2 Align] epoch 45/50, loss=1.0301\n",
      "[RQ2 Align] epoch 46/50, loss=1.0301\n",
      "[RQ2 Align] epoch 47/50, loss=1.0379\n",
      "[RQ2 Align] epoch 48/50, loss=1.0301\n",
      "[RQ2 Align] epoch 49/50, loss=1.0029\n",
      "[RQ2 Align] epoch 50/50, loss=1.0086\n",
      "Saved RQ2 checkpoints for resnet50.\n",
      "\n",
      " RQ2: vgg16 encoder \n",
      "Loaded 12575 images from /users/PAS2985/tingle9/dataset/footprint_patches/train, 117 classes.\n",
      "Loaded contrastive encoder : vgg16\n",
      "[RQ2 Align] epoch 1/50, loss=3.8190\n",
      "[RQ2 Align] epoch 2/50, loss=3.4867\n",
      "[RQ2 Align] epoch 3/50, loss=3.3641\n",
      "[RQ2 Align] epoch 4/50, loss=3.2780\n",
      "[RQ2 Align] epoch 5/50, loss=3.2207\n",
      "[RQ2 Align] epoch 6/50, loss=3.1516\n",
      "[RQ2 Align] epoch 7/50, loss=3.1134\n",
      "[RQ2 Align] epoch 8/50, loss=3.0671\n",
      "[RQ2 Align] epoch 9/50, loss=3.0274\n",
      "[RQ2 Align] epoch 10/50, loss=2.9797\n",
      "[RQ2 Align] epoch 11/50, loss=2.9421\n",
      "[RQ2 Align] epoch 12/50, loss=2.8923\n",
      "[RQ2 Align] epoch 13/50, loss=2.8529\n",
      "[RQ2 Align] epoch 14/50, loss=2.8159\n",
      "[RQ2 Align] epoch 15/50, loss=2.7770\n",
      "[RQ2 Align] epoch 16/50, loss=2.7310\n",
      "[RQ2 Align] epoch 17/50, loss=2.6990\n",
      "[RQ2 Align] epoch 18/50, loss=2.6596\n",
      "[RQ2 Align] epoch 19/50, loss=2.6338\n",
      "[RQ2 Align] epoch 20/50, loss=2.5926\n",
      "[RQ2 Align] epoch 21/50, loss=2.5456\n",
      "[RQ2 Align] epoch 22/50, loss=2.5249\n",
      "[RQ2 Align] epoch 23/50, loss=2.4923\n",
      "[RQ2 Align] epoch 24/50, loss=2.4678\n",
      "[RQ2 Align] epoch 25/50, loss=2.4188\n",
      "[RQ2 Align] epoch 26/50, loss=2.3925\n",
      "[RQ2 Align] epoch 27/50, loss=2.3813\n",
      "[RQ2 Align] epoch 28/50, loss=2.3461\n",
      "[RQ2 Align] epoch 29/50, loss=2.3420\n",
      "[RQ2 Align] epoch 30/50, loss=2.2897\n",
      "[RQ2 Align] epoch 31/50, loss=2.2766\n",
      "[RQ2 Align] epoch 32/50, loss=2.2603\n",
      "[RQ2 Align] epoch 33/50, loss=2.2170\n",
      "[RQ2 Align] epoch 34/50, loss=2.2108\n",
      "[RQ2 Align] epoch 35/50, loss=2.1985\n",
      "[RQ2 Align] epoch 36/50, loss=2.1678\n",
      "[RQ2 Align] epoch 37/50, loss=2.1403\n",
      "[RQ2 Align] epoch 38/50, loss=2.1245\n",
      "[RQ2 Align] epoch 39/50, loss=2.1096\n",
      "[RQ2 Align] epoch 40/50, loss=2.1013\n",
      "[RQ2 Align] epoch 41/50, loss=2.0884\n",
      "[RQ2 Align] epoch 42/50, loss=2.0737\n",
      "[RQ2 Align] epoch 43/50, loss=2.0446\n",
      "[RQ2 Align] epoch 44/50, loss=2.0267\n",
      "[RQ2 Align] epoch 45/50, loss=2.0165\n",
      "[RQ2 Align] epoch 46/50, loss=1.9941\n",
      "[RQ2 Align] epoch 47/50, loss=1.9980\n",
      "[RQ2 Align] epoch 48/50, loss=1.9750\n",
      "[RQ2 Align] epoch 49/50, loss=1.9711\n",
      "[RQ2 Align] epoch 50/50, loss=1.9578\n",
      "Saved RQ2 checkpoints for vgg16.\n",
      "\n",
      " RQ2: vit_b_16 encoder \n",
      "Loaded 12575 images from /users/PAS2985/tingle9/dataset/footprint_patches/train, 117 classes.\n",
      "Loaded contrastive encoder : vit_b_16\n",
      "[RQ2 Align] epoch 1/50, loss=3.8394\n",
      "[RQ2 Align] epoch 2/50, loss=3.6844\n",
      "[RQ2 Align] epoch 3/50, loss=3.6103\n",
      "[RQ2 Align] epoch 4/50, loss=3.5579\n",
      "[RQ2 Align] epoch 5/50, loss=3.5106\n",
      "[RQ2 Align] epoch 6/50, loss=3.4838\n",
      "[RQ2 Align] epoch 7/50, loss=3.4410\n",
      "[RQ2 Align] epoch 8/50, loss=3.4139\n",
      "[RQ2 Align] epoch 9/50, loss=3.3818\n",
      "[RQ2 Align] epoch 10/50, loss=3.3615\n",
      "[RQ2 Align] epoch 11/50, loss=3.3417\n",
      "[RQ2 Align] epoch 12/50, loss=3.3120\n",
      "[RQ2 Align] epoch 13/50, loss=3.2966\n",
      "[RQ2 Align] epoch 14/50, loss=3.2742\n",
      "[RQ2 Align] epoch 15/50, loss=3.2608\n",
      "[RQ2 Align] epoch 16/50, loss=3.2405\n",
      "[RQ2 Align] epoch 17/50, loss=3.2191\n",
      "[RQ2 Align] epoch 18/50, loss=3.2044\n",
      "[RQ2 Align] epoch 19/50, loss=3.1965\n",
      "[RQ2 Align] epoch 20/50, loss=3.1767\n",
      "[RQ2 Align] epoch 21/50, loss=3.1659\n",
      "[RQ2 Align] epoch 22/50, loss=3.1565\n",
      "[RQ2 Align] epoch 23/50, loss=3.1323\n",
      "[RQ2 Align] epoch 24/50, loss=3.1300\n",
      "[RQ2 Align] epoch 25/50, loss=3.1160\n",
      "[RQ2 Align] epoch 26/50, loss=3.0981\n",
      "[RQ2 Align] epoch 27/50, loss=3.0963\n",
      "[RQ2 Align] epoch 28/50, loss=3.0786\n",
      "[RQ2 Align] epoch 29/50, loss=3.0667\n",
      "[RQ2 Align] epoch 30/50, loss=3.0663\n",
      "[RQ2 Align] epoch 31/50, loss=3.0586\n",
      "[RQ2 Align] epoch 32/50, loss=3.0347\n",
      "[RQ2 Align] epoch 33/50, loss=3.0225\n",
      "[RQ2 Align] epoch 34/50, loss=3.0186\n",
      "[RQ2 Align] epoch 35/50, loss=3.0166\n",
      "[RQ2 Align] epoch 36/50, loss=2.9948\n",
      "[RQ2 Align] epoch 37/50, loss=2.9990\n",
      "[RQ2 Align] epoch 38/50, loss=2.9864\n",
      "[RQ2 Align] epoch 39/50, loss=2.9877\n",
      "[RQ2 Align] epoch 40/50, loss=2.9739\n",
      "[RQ2 Align] epoch 41/50, loss=2.9570\n",
      "[RQ2 Align] epoch 42/50, loss=2.9658\n",
      "[RQ2 Align] epoch 43/50, loss=2.9455\n",
      "[RQ2 Align] epoch 44/50, loss=2.9463\n",
      "[RQ2 Align] epoch 45/50, loss=2.9517\n",
      "[RQ2 Align] epoch 46/50, loss=2.9427\n",
      "[RQ2 Align] epoch 47/50, loss=2.9235\n",
      "[RQ2 Align] epoch 48/50, loss=2.9178\n",
      "[RQ2 Align] epoch 49/50, loss=2.9217\n",
      "[RQ2 Align] epoch 50/50, loss=2.8992\n",
      "Saved RQ2 checkpoints for vit_b_16.\n"
     ]
    }
   ],
   "source": [
    "PATCH_ROOT = Path(\"/users/PAS2985/tingle9/dataset/footprint_patches\")\n",
    "\n",
    "proj_dim = 128\n",
    "batch_size = 64\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(\"\\n RQ2 : small CNN encoder \")\n",
    "image_size = 128\n",
    "\n",
    "rq2_train_transform = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(image_size, scale=(0.6, 1.0)),\n",
    "    transforms.RandomHorizontalFlip(0.5),\n",
    "    transforms.ColorJitter(0.2, 0.2, 0.2, 0.05),\n",
    "    transforms.RandomGrayscale(p=0.1),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "train_ds_cnn = FootprintPatchDataset(PATCH_ROOT / \"train\", transform=rq2_train_transform)\n",
    "num_classes = len(train_ds_cnn.idx_to_class)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_ds_cnn,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=8,\n",
    "    drop_last=True,\n",
    "    pin_memory=True,\n",
    "    persistent_workers=True\n",
    ")\n",
    "\n",
    "# 1) Load pre-trained encoder\n",
    "CONTRASTIVE_CKPT = \"footprint_encoder_contrastive.pth\"\n",
    "encoder = FootprintEncoder(feature_dim=256)\n",
    "encoder.load_state_dict(torch.load(CONTRASTIVE_CKPT, map_location=\"cpu\"))\n",
    "\n",
    "img_proj = ImageProjector(in_dim=256, out_dim=proj_dim)\n",
    "txt_enc = TextTokenEncoder(num_classes, proj_dim)\n",
    "\n",
    "# 2) RQ2 alignment (encoder freeze)\n",
    "train_rq2_alignment(\n",
    "    encoder, img_proj, txt_enc,\n",
    "    train_loader, device,\n",
    "    epochs=50, lr=1e-3,\n",
    "    freeze_encoder=True\n",
    ")\n",
    "\n",
    "torch.save(img_proj.state_dict(), \"rq2_CNN_image_projector.pth\")\n",
    "torch.save(txt_enc.state_dict(), \"rq2_CNN_text_token_encoder.pth\")\n",
    "print(\"saved RQ2 CNN checkpoints. \")\n",
    "\n",
    "IMAGENET_MEAN = (0.485, 0.456, 0.406)\n",
    "IMAGENET_STD  = (0.229, 0.224, 0.225)\n",
    "\n",
    "backbones = [\"resnet50\", \"vgg16\", \"vit_b_16\"]\n",
    "\n",
    "for backbone in backbones:\n",
    "    \n",
    "    print(f\"\\n RQ2: {backbone} encoder \")\n",
    "    CONTRASTIVE_CKPT = f\"encoder_{backbone}_contrastive.pth\"\n",
    "    \n",
    "    if backbone == \"vit_b_16\":\n",
    "        image_size = 224\n",
    "    else :\n",
    "        image_size = 128\n",
    "    \n",
    "    rq2_train_transform = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(image_size, scale=(0.6, 1.0)),\n",
    "    transforms.RandomHorizontalFlip(0.5),\n",
    "    transforms.ColorJitter(0.2, 0.2, 0.2, 0.05),\n",
    "    transforms.RandomGrayscale(p=0.1),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD)\n",
    "])\n",
    "    train_ds = FootprintPatchDataset(PATCH_ROOT / \"train\", transform=rq2_train_transform)\n",
    "    \n",
    "    train_loader = DataLoader(\n",
    "    train_ds,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=8,\n",
    "    drop_last=True,\n",
    "    pin_memory=True,\n",
    "    persistent_workers=True\n",
    ")\n",
    "    \n",
    "    encoder = load_contrastive_encoder(backbone, CONTRASTIVE_CKPT, device)\n",
    "\n",
    "    img_proj = ImageProjector(in_dim=encoder.out_dim, out_dim=proj_dim)\n",
    "    txt_enc = TextTokenEncoder(num_classes, proj_dim)\n",
    "\n",
    "    # RQ2 alignment (encoder freeze)\n",
    "    train_rq2_alignment(\n",
    "        encoder, img_proj, txt_enc,\n",
    "        train_loader, device,\n",
    "        epochs=50, lr=1e-3,\n",
    "        freeze_encoder=True\n",
    "    )\n",
    "\n",
    "    torch.save(img_proj.state_dict(), f\"rq2_{backbone}_image_projector.pth\")\n",
    "    torch.save(txt_enc.state_dict(), f\"rq2_{backbone}_text_token_encoder.pth\")\n",
    "    print(f\"Saved RQ2 checkpoints for {backbone}.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "awsenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
