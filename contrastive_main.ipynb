{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a19f25be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "97809aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FootprintPatchDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Classification-style dataset for cropped footprint patches.\n",
    "    Assumes directory structure:\n",
    "      root_dir/\n",
    "        class_name0/\n",
    "          *.jpg\n",
    "        class_name1/\n",
    "          *.jpg\n",
    "        ...\n",
    "    \"\"\"\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = Path(root_dir)\n",
    "        self.transform = transform\n",
    "\n",
    "        self.image_paths = []\n",
    "        self.labels = []\n",
    "        self.class_to_idx = {}\n",
    "        self.idx_to_class = []\n",
    "\n",
    "        for class_idx, class_name in enumerate(sorted(os.listdir(self.root_dir))):\n",
    "            class_path = self.root_dir / class_name\n",
    "            if not class_path.is_dir():\n",
    "                continue\n",
    "\n",
    "            self.class_to_idx[class_name] = class_idx\n",
    "            self.idx_to_class.append(class_name)\n",
    "\n",
    "            for fname in os.listdir(class_path):\n",
    "                if fname.lower().endswith((\".jpg\", \".jpeg\", \".png\")):\n",
    "                    self.image_paths.append(class_path / fname)\n",
    "                    self.labels.append(class_idx)\n",
    "\n",
    "        print(f\"Loaded {len(self.image_paths)} images \"\n",
    "              f\"from {self.root_dir}, {len(self.idx_to_class)} classes.\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        label = self.labels[idx]\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label\n",
    "\n",
    "\n",
    "class ContrastiveTransform:\n",
    "    \"\"\"\n",
    "    Wraps a base transform and returns TWO augmented views.\n",
    "    \"\"\"\n",
    "    def __init__(self, base_transform):\n",
    "        self.base_transform = base_transform\n",
    "\n",
    "    def __call__(self, x):\n",
    "        xi = self.base_transform(x)\n",
    "        xj = self.base_transform(x)\n",
    "        return xi, xj\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "204c257b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 12575 images from C:\\Users\\Kdbro\\OneDrive\\Desktop\\OSU\\Fall 2025\\Neural Networks\\Contrastive-and-Attribute-Aligned-Representation-Learning-for-Animal-Footprint\\dataset\\footprint_patches\\train, 117 classes.\n",
      "torch.Size([64, 3, 128, 128]) torch.Size([64, 3, 128, 128])\n",
      "torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "PATCH_ROOT = Path(\n",
    "    r\"C:\\Users\\Kdbro\\OneDrive\\Desktop\\OSU\\Fall 2025\\Neural Networks\\Contrastive-and-Attribute-Aligned-Representation-Learning-for-Animal-Footprint\\dataset\\footprint_patches\"\n",
    ")\n",
    "image_size = 128\n",
    "\n",
    "base_transform = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(image_size, scale=(0.6, 1.0)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomApply([\n",
    "        transforms.ColorJitter(brightness=0.4, contrast=0.4,\n",
    "                               saturation=0.4, hue=0.1)\n",
    "    ], p=0.8),\n",
    "    transforms.RandomGrayscale(p=0.2),\n",
    "    transforms.GaussianBlur(kernel_size=9, sigma=(0.1, 2.0)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "contrastive_transform = ContrastiveTransform(base_transform)\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "train_dataset_contrastive = FootprintPatchDataset(\n",
    "    PATCH_ROOT / \"train\",\n",
    "    transform=contrastive_transform\n",
    ")\n",
    "\n",
    "contrastive_loader = DataLoader(\n",
    "    train_dataset_contrastive,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    "    drop_last=True\n",
    ")\n",
    "\n",
    "batch_views, labels = next(iter(contrastive_loader))\n",
    "xi, xj = batch_views\n",
    "print(xi.shape, xj.shape)  # should be [batch_size, 3, 128, 128]\n",
    "print(labels.shape)        # [batch_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "95b25764",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FootprintEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple CNN encoder for footprint images.\n",
    "    Input: (B, 3, 128, 128)\n",
    "    Output: (B, feature_dim)\n",
    "    \"\"\"\n",
    "    def __init__(self, feature_dim=256):\n",
    "        super().__init__()\n",
    "        self.feature_dim = feature_dim\n",
    "\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2)  # 128 -> 64\n",
    "        )\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2)  # 64 -> 32\n",
    "        )\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2)  # 32 -> 16\n",
    "        )\n",
    "        self.conv4 = nn.Sequential(\n",
    "            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.AdaptiveAvgPool2d((1, 1))\n",
    "        )\n",
    "\n",
    "        self.fc = nn.Linear(256, feature_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.conv4(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eadcee03",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FootprintEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple CNN encoder for footprint images.\n",
    "    Input: (B, 3, 128, 128)\n",
    "    Output: (B, feature_dim)\n",
    "    \"\"\"\n",
    "    def __init__(self, feature_dim=256):\n",
    "        super().__init__()\n",
    "        self.feature_dim = feature_dim\n",
    "\n",
    "        # Conv block 1: 3 -> 32\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2)  # 128 -> 64\n",
    "        )\n",
    "        # Conv block 2: 32 -> 64\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2)  # 64 -> 32\n",
    "        )\n",
    "        # Conv block 3: 64 -> 128\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2)  # 32 -> 16\n",
    "        )\n",
    "        # Conv block 4: 128 -> 256\n",
    "        self.conv4 = nn.Sequential(\n",
    "            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            # Global average pooling to (B, 256, 1, 1)\n",
    "            nn.AdaptiveAvgPool2d((1, 1))\n",
    "        )\n",
    "\n",
    "        # Final linear layer to get a compact feature vector\n",
    "        self.fc = nn.Linear(256, feature_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)   # (B, 32, 64, 64)\n",
    "        x = self.conv2(x)   # (B, 64, 32, 32)\n",
    "        x = self.conv3(x)   # (B, 128, 16, 16)\n",
    "        x = self.conv4(x)   # (B, 256, 1, 1)\n",
    "        x = x.view(x.size(0), -1)  # (B, 256)\n",
    "        x = self.fc(x)      # (B, feature_dim)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ef0fd42b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProjectionHead(nn.Module):\n",
    "    def __init__(self, feature_dim=256, projection_dim=128):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(feature_dim, feature_dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(feature_dim, projection_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class ContrastiveModel(nn.Module):\n",
    "    def __init__(self, feature_dim=256, projection_dim=128):\n",
    "        super().__init__()\n",
    "        self.encoder = FootprintEncoder(feature_dim=feature_dim)\n",
    "        self.projection_head = ProjectionHead(\n",
    "            feature_dim=feature_dim,\n",
    "            projection_dim=projection_dim\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.encoder(x)\n",
    "        z = self.projection_head(h)\n",
    "        return h, z\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6524417c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nt_xent_loss(z_i, z_j, temperature=0.5):\n",
    "    \"\"\"\n",
    "    SimCLR-style NT-Xent loss.\n",
    "    \"\"\"\n",
    "    device = z_i.device\n",
    "    batch_size = z_i.size(0)\n",
    "\n",
    "    z_i = F.normalize(z_i, dim=1)\n",
    "    z_j = F.normalize(z_j, dim=1)\n",
    "\n",
    "    z = torch.cat([z_i, z_j], dim=0)  # (2N, d)\n",
    "\n",
    "    sim = torch.matmul(z, z.T) / temperature\n",
    "\n",
    "    mask = torch.eye(2 * batch_size, dtype=torch.bool, device=device)\n",
    "    sim = sim.masked_fill(mask, float('-inf'))\n",
    "\n",
    "    pos_indices = (torch.arange(2 * batch_size, device=device)\n",
    "                   + batch_size) % (2 * batch_size)\n",
    "\n",
    "    positives = sim[torch.arange(2 * batch_size, device=device), pos_indices]\n",
    "    log_sum_exp = torch.logsumexp(sim, dim=1)\n",
    "\n",
    "    loss = - (positives - log_sum_exp).mean()\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6dec1053",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_contrastive(model,\n",
    "                      dataloader,\n",
    "                      optimizer,\n",
    "                      device,\n",
    "                      epochs=50,\n",
    "                      temperature=0.5):\n",
    "    model.train()\n",
    "    print(\">>> Entered train_contrastive\")  # DEBUG\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0.0\n",
    "        num_batches = 0\n",
    "        print(f\"Starting epoch {epoch+1}/{epochs}\")  # DEBUG\n",
    "\n",
    "        for batch_idx, (batch_views, _) in enumerate(dataloader):\n",
    "            xi, xj = batch_views\n",
    "            xi = xi.to(device)\n",
    "            xj = xj.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            _, zi = model(xi)\n",
    "            _, zj = model(xj)\n",
    "\n",
    "            loss = nt_xent_loss(zi, zj, temperature=temperature)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            num_batches += 1\n",
    "\n",
    "            # Print every 20 batches so you see progress\n",
    "            if (batch_idx + 1) % 20 == 0:\n",
    "                print(f\"  Epoch {epoch+1}, batch {batch_idx+1}, \"\n",
    "                      f\"loss={loss.item():.4f}\")\n",
    "\n",
    "        avg_loss = total_loss / max(1, num_batches)\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}] - Contrastive loss: {avg_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b210b799",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Entered train_contrastive\n",
      "Starting epoch 1/50\n",
      "  Epoch 1, batch 20, loss=4.0820\n",
      "  Epoch 1, batch 40, loss=4.0583\n",
      "  Epoch 1, batch 60, loss=4.1995\n",
      "  Epoch 1, batch 80, loss=4.1837\n",
      "  Epoch 1, batch 100, loss=4.0254\n",
      "  Epoch 1, batch 120, loss=3.9178\n",
      "  Epoch 1, batch 140, loss=3.9302\n",
      "  Epoch 1, batch 160, loss=3.7761\n",
      "  Epoch 1, batch 180, loss=3.8408\n",
      "Epoch [1/50] - Contrastive loss: 4.0086\n",
      "Starting epoch 2/50\n",
      "  Epoch 2, batch 20, loss=3.8157\n",
      "  Epoch 2, batch 40, loss=3.7793\n",
      "  Epoch 2, batch 60, loss=3.7698\n",
      "  Epoch 2, batch 80, loss=3.7840\n",
      "  Epoch 2, batch 100, loss=3.7207\n",
      "  Epoch 2, batch 120, loss=3.7616\n",
      "  Epoch 2, batch 140, loss=3.6072\n",
      "  Epoch 2, batch 160, loss=3.6127\n",
      "  Epoch 2, batch 180, loss=3.6929\n",
      "Epoch [2/50] - Contrastive loss: 3.7394\n",
      "Starting epoch 3/50\n",
      "  Epoch 3, batch 20, loss=3.5766\n",
      "  Epoch 3, batch 40, loss=3.6986\n",
      "  Epoch 3, batch 60, loss=3.6591\n",
      "  Epoch 3, batch 80, loss=3.6491\n",
      "  Epoch 3, batch 100, loss=3.5934\n",
      "  Epoch 3, batch 120, loss=3.5656\n",
      "  Epoch 3, batch 140, loss=3.6341\n",
      "  Epoch 3, batch 160, loss=3.5251\n",
      "  Epoch 3, batch 180, loss=3.4435\n",
      "Epoch [3/50] - Contrastive loss: 3.6077\n",
      "Starting epoch 4/50\n",
      "  Epoch 4, batch 20, loss=3.6221\n",
      "  Epoch 4, batch 40, loss=3.4943\n",
      "  Epoch 4, batch 60, loss=3.4979\n",
      "  Epoch 4, batch 80, loss=3.4259\n",
      "  Epoch 4, batch 100, loss=3.4706\n",
      "  Epoch 4, batch 120, loss=3.4776\n",
      "  Epoch 4, batch 140, loss=3.4525\n",
      "  Epoch 4, batch 160, loss=3.3711\n",
      "  Epoch 4, batch 180, loss=3.4355\n",
      "Epoch [4/50] - Contrastive loss: 3.4606\n",
      "Starting epoch 5/50\n",
      "  Epoch 5, batch 20, loss=3.4385\n",
      "  Epoch 5, batch 40, loss=3.4110\n",
      "  Epoch 5, batch 60, loss=3.3834\n",
      "  Epoch 5, batch 80, loss=3.4308\n",
      "  Epoch 5, batch 100, loss=3.4022\n",
      "  Epoch 5, batch 120, loss=3.4706\n",
      "  Epoch 5, batch 140, loss=3.4727\n",
      "  Epoch 5, batch 160, loss=3.3913\n",
      "  Epoch 5, batch 180, loss=3.4297\n",
      "Epoch [5/50] - Contrastive loss: 3.4117\n",
      "Starting epoch 6/50\n",
      "  Epoch 6, batch 20, loss=3.3588\n",
      "  Epoch 6, batch 40, loss=3.3468\n",
      "  Epoch 6, batch 60, loss=3.3871\n",
      "  Epoch 6, batch 80, loss=3.3608\n",
      "  Epoch 6, batch 100, loss=3.4073\n",
      "  Epoch 6, batch 120, loss=3.3667\n",
      "  Epoch 6, batch 140, loss=3.4078\n",
      "  Epoch 6, batch 160, loss=3.3095\n",
      "  Epoch 6, batch 180, loss=3.3460\n",
      "Epoch [6/50] - Contrastive loss: 3.3690\n",
      "Starting epoch 7/50\n",
      "  Epoch 7, batch 20, loss=3.3511\n",
      "  Epoch 7, batch 40, loss=3.4388\n",
      "  Epoch 7, batch 60, loss=3.3183\n",
      "  Epoch 7, batch 80, loss=3.3570\n",
      "  Epoch 7, batch 100, loss=3.3662\n",
      "  Epoch 7, batch 120, loss=3.3072\n",
      "  Epoch 7, batch 140, loss=3.3693\n",
      "  Epoch 7, batch 160, loss=3.3718\n",
      "  Epoch 7, batch 180, loss=3.2800\n",
      "Epoch [7/50] - Contrastive loss: 3.3467\n",
      "Starting epoch 8/50\n",
      "  Epoch 8, batch 20, loss=3.3328\n",
      "  Epoch 8, batch 40, loss=3.3567\n",
      "  Epoch 8, batch 60, loss=3.4069\n",
      "  Epoch 8, batch 80, loss=3.3071\n",
      "  Epoch 8, batch 100, loss=3.3329\n",
      "  Epoch 8, batch 120, loss=3.3444\n",
      "  Epoch 8, batch 140, loss=3.3140\n",
      "  Epoch 8, batch 160, loss=3.2877\n",
      "  Epoch 8, batch 180, loss=3.3171\n",
      "Epoch [8/50] - Contrastive loss: 3.3353\n",
      "Starting epoch 9/50\n",
      "  Epoch 9, batch 20, loss=3.2948\n",
      "  Epoch 9, batch 40, loss=3.3312\n",
      "  Epoch 9, batch 60, loss=3.2992\n",
      "  Epoch 9, batch 80, loss=3.2860\n",
      "  Epoch 9, batch 100, loss=3.3159\n",
      "  Epoch 9, batch 120, loss=3.3137\n",
      "  Epoch 9, batch 140, loss=3.3530\n",
      "  Epoch 9, batch 160, loss=3.2853\n",
      "  Epoch 9, batch 180, loss=3.3659\n",
      "Epoch [9/50] - Contrastive loss: 3.3127\n",
      "Starting epoch 10/50\n",
      "  Epoch 10, batch 20, loss=3.3506\n",
      "  Epoch 10, batch 40, loss=3.3061\n",
      "  Epoch 10, batch 60, loss=3.3359\n",
      "  Epoch 10, batch 80, loss=3.2739\n",
      "  Epoch 10, batch 100, loss=3.3350\n",
      "  Epoch 10, batch 120, loss=3.3614\n",
      "  Epoch 10, batch 140, loss=3.2860\n",
      "  Epoch 10, batch 160, loss=3.3337\n",
      "  Epoch 10, batch 180, loss=3.3232\n",
      "Epoch [10/50] - Contrastive loss: 3.3064\n",
      "Starting epoch 11/50\n",
      "  Epoch 11, batch 20, loss=3.3449\n",
      "  Epoch 11, batch 40, loss=3.3740\n",
      "  Epoch 11, batch 60, loss=3.2649\n",
      "  Epoch 11, batch 80, loss=3.2263\n",
      "  Epoch 11, batch 100, loss=3.2548\n",
      "  Epoch 11, batch 120, loss=3.2433\n",
      "  Epoch 11, batch 140, loss=3.3833\n",
      "  Epoch 11, batch 160, loss=3.2825\n",
      "  Epoch 11, batch 180, loss=3.2973\n",
      "Epoch [11/50] - Contrastive loss: 3.2886\n",
      "Starting epoch 12/50\n",
      "  Epoch 12, batch 20, loss=3.3528\n",
      "  Epoch 12, batch 40, loss=3.2959\n",
      "  Epoch 12, batch 60, loss=3.2981\n",
      "  Epoch 12, batch 80, loss=3.2923\n",
      "  Epoch 12, batch 100, loss=3.2557\n",
      "  Epoch 12, batch 120, loss=3.3338\n",
      "  Epoch 12, batch 140, loss=3.3006\n",
      "  Epoch 12, batch 160, loss=3.2701\n",
      "  Epoch 12, batch 180, loss=3.2416\n",
      "Epoch [12/50] - Contrastive loss: 3.2867\n",
      "Starting epoch 13/50\n",
      "  Epoch 13, batch 20, loss=3.2701\n",
      "  Epoch 13, batch 40, loss=3.2550\n",
      "  Epoch 13, batch 60, loss=3.2576\n",
      "  Epoch 13, batch 80, loss=3.2391\n",
      "  Epoch 13, batch 100, loss=3.2278\n",
      "  Epoch 13, batch 120, loss=3.2579\n",
      "  Epoch 13, batch 140, loss=3.2777\n",
      "  Epoch 13, batch 160, loss=3.3095\n",
      "  Epoch 13, batch 180, loss=3.2392\n",
      "Epoch [13/50] - Contrastive loss: 3.2731\n",
      "Starting epoch 14/50\n",
      "  Epoch 14, batch 20, loss=3.2641\n",
      "  Epoch 14, batch 40, loss=3.2389\n",
      "  Epoch 14, batch 60, loss=3.2923\n",
      "  Epoch 14, batch 80, loss=3.3565\n",
      "  Epoch 14, batch 100, loss=3.2810\n",
      "  Epoch 14, batch 120, loss=3.2518\n",
      "  Epoch 14, batch 140, loss=3.2459\n",
      "  Epoch 14, batch 160, loss=3.2419\n",
      "  Epoch 14, batch 180, loss=3.1977\n",
      "Epoch [14/50] - Contrastive loss: 3.2733\n",
      "Starting epoch 15/50\n",
      "  Epoch 15, batch 20, loss=3.2927\n",
      "  Epoch 15, batch 40, loss=3.3211\n",
      "  Epoch 15, batch 60, loss=3.2662\n",
      "  Epoch 15, batch 80, loss=3.2315\n",
      "  Epoch 15, batch 100, loss=3.2895\n",
      "  Epoch 15, batch 120, loss=3.2375\n",
      "  Epoch 15, batch 140, loss=3.2599\n",
      "  Epoch 15, batch 160, loss=3.2962\n",
      "  Epoch 15, batch 180, loss=3.2309\n",
      "Epoch [15/50] - Contrastive loss: 3.2663\n",
      "Starting epoch 16/50\n",
      "  Epoch 16, batch 20, loss=3.3246\n",
      "  Epoch 16, batch 40, loss=3.2759\n",
      "  Epoch 16, batch 60, loss=3.2575\n",
      "  Epoch 16, batch 80, loss=3.2774\n",
      "  Epoch 16, batch 100, loss=3.2535\n",
      "  Epoch 16, batch 120, loss=3.2481\n",
      "  Epoch 16, batch 140, loss=3.2568\n",
      "  Epoch 16, batch 160, loss=3.2998\n",
      "  Epoch 16, batch 180, loss=3.2643\n",
      "Epoch [16/50] - Contrastive loss: 3.2637\n",
      "Starting epoch 17/50\n",
      "  Epoch 17, batch 20, loss=3.2041\n",
      "  Epoch 17, batch 40, loss=3.2368\n",
      "  Epoch 17, batch 60, loss=3.2777\n",
      "  Epoch 17, batch 80, loss=3.2823\n",
      "  Epoch 17, batch 100, loss=3.2324\n",
      "  Epoch 17, batch 120, loss=3.2153\n",
      "  Epoch 17, batch 140, loss=3.2633\n",
      "  Epoch 17, batch 160, loss=3.2207\n",
      "  Epoch 17, batch 180, loss=3.2402\n",
      "Epoch [17/50] - Contrastive loss: 3.2527\n",
      "Starting epoch 18/50\n",
      "  Epoch 18, batch 20, loss=3.2437\n",
      "  Epoch 18, batch 40, loss=3.2773\n",
      "  Epoch 18, batch 60, loss=3.2459\n",
      "  Epoch 18, batch 80, loss=3.2714\n",
      "  Epoch 18, batch 100, loss=3.2583\n",
      "  Epoch 18, batch 120, loss=3.2047\n",
      "  Epoch 18, batch 140, loss=3.2460\n",
      "  Epoch 18, batch 160, loss=3.2713\n",
      "  Epoch 18, batch 180, loss=3.2288\n",
      "Epoch [18/50] - Contrastive loss: 3.2466\n",
      "Starting epoch 19/50\n",
      "  Epoch 19, batch 20, loss=3.2825\n",
      "  Epoch 19, batch 40, loss=3.2375\n",
      "  Epoch 19, batch 60, loss=3.2183\n",
      "  Epoch 19, batch 80, loss=3.2747\n",
      "  Epoch 19, batch 100, loss=3.2331\n",
      "  Epoch 19, batch 120, loss=3.2124\n",
      "  Epoch 19, batch 140, loss=3.2671\n",
      "  Epoch 19, batch 160, loss=3.2438\n",
      "  Epoch 19, batch 180, loss=3.3176\n",
      "Epoch [19/50] - Contrastive loss: 3.2447\n",
      "Starting epoch 20/50\n",
      "  Epoch 20, batch 20, loss=3.2372\n",
      "  Epoch 20, batch 40, loss=3.2283\n",
      "  Epoch 20, batch 60, loss=3.2114\n",
      "  Epoch 20, batch 80, loss=3.2707\n",
      "  Epoch 20, batch 100, loss=3.2978\n",
      "  Epoch 20, batch 120, loss=3.2634\n",
      "  Epoch 20, batch 140, loss=3.2645\n",
      "  Epoch 20, batch 160, loss=3.1963\n",
      "  Epoch 20, batch 180, loss=3.1869\n",
      "Epoch [20/50] - Contrastive loss: 3.2414\n",
      "Starting epoch 21/50\n",
      "  Epoch 21, batch 20, loss=3.2614\n",
      "  Epoch 21, batch 40, loss=3.2201\n",
      "  Epoch 21, batch 60, loss=3.2066\n",
      "  Epoch 21, batch 80, loss=3.2404\n",
      "  Epoch 21, batch 100, loss=3.2176\n",
      "  Epoch 21, batch 120, loss=3.2433\n",
      "  Epoch 21, batch 140, loss=3.2651\n",
      "  Epoch 21, batch 160, loss=3.2260\n",
      "  Epoch 21, batch 180, loss=3.2750\n",
      "Epoch [21/50] - Contrastive loss: 3.2336\n",
      "Starting epoch 22/50\n",
      "  Epoch 22, batch 20, loss=3.2861\n",
      "  Epoch 22, batch 40, loss=3.2307\n",
      "  Epoch 22, batch 60, loss=3.2140\n",
      "  Epoch 22, batch 80, loss=3.2256\n",
      "  Epoch 22, batch 100, loss=3.2372\n",
      "  Epoch 22, batch 120, loss=3.2461\n",
      "  Epoch 22, batch 140, loss=3.2313\n",
      "  Epoch 22, batch 160, loss=3.2000\n",
      "  Epoch 22, batch 180, loss=3.2155\n",
      "Epoch [22/50] - Contrastive loss: 3.2319\n",
      "Starting epoch 23/50\n",
      "  Epoch 23, batch 20, loss=3.2718\n",
      "  Epoch 23, batch 40, loss=3.1557\n",
      "  Epoch 23, batch 60, loss=3.2505\n",
      "  Epoch 23, batch 80, loss=3.2066\n",
      "  Epoch 23, batch 100, loss=3.3185\n",
      "  Epoch 23, batch 120, loss=3.2137\n",
      "  Epoch 23, batch 140, loss=3.2370\n",
      "  Epoch 23, batch 160, loss=3.1955\n",
      "  Epoch 23, batch 180, loss=3.2424\n",
      "Epoch [23/50] - Contrastive loss: 3.2276\n",
      "Starting epoch 24/50\n",
      "  Epoch 24, batch 20, loss=3.2149\n",
      "  Epoch 24, batch 40, loss=3.2276\n",
      "  Epoch 24, batch 60, loss=3.2032\n",
      "  Epoch 24, batch 80, loss=3.1629\n",
      "  Epoch 24, batch 100, loss=3.1893\n",
      "  Epoch 24, batch 120, loss=3.2051\n",
      "  Epoch 24, batch 140, loss=3.2326\n",
      "  Epoch 24, batch 160, loss=3.3032\n",
      "  Epoch 24, batch 180, loss=3.2467\n",
      "Epoch [24/50] - Contrastive loss: 3.2193\n",
      "Starting epoch 25/50\n",
      "  Epoch 25, batch 20, loss=3.1892\n",
      "  Epoch 25, batch 40, loss=3.2262\n",
      "  Epoch 25, batch 60, loss=3.1916\n",
      "  Epoch 25, batch 80, loss=3.3118\n",
      "  Epoch 25, batch 100, loss=3.2680\n",
      "  Epoch 25, batch 120, loss=3.1814\n",
      "  Epoch 25, batch 140, loss=3.2726\n",
      "  Epoch 25, batch 160, loss=3.2472\n",
      "  Epoch 25, batch 180, loss=3.1797\n",
      "Epoch [25/50] - Contrastive loss: 3.2153\n",
      "Starting epoch 26/50\n",
      "  Epoch 26, batch 20, loss=3.2593\n",
      "  Epoch 26, batch 40, loss=3.2583\n",
      "  Epoch 26, batch 60, loss=3.2472\n",
      "  Epoch 26, batch 80, loss=3.2247\n",
      "  Epoch 26, batch 100, loss=3.1879\n",
      "  Epoch 26, batch 120, loss=3.2202\n",
      "  Epoch 26, batch 140, loss=3.1923\n",
      "  Epoch 26, batch 160, loss=3.1979\n",
      "  Epoch 26, batch 180, loss=3.1826\n",
      "Epoch [26/50] - Contrastive loss: 3.2132\n",
      "Starting epoch 27/50\n",
      "  Epoch 27, batch 20, loss=3.1895\n",
      "  Epoch 27, batch 40, loss=3.2504\n",
      "  Epoch 27, batch 60, loss=3.2181\n",
      "  Epoch 27, batch 80, loss=3.1938\n",
      "  Epoch 27, batch 100, loss=3.2112\n",
      "  Epoch 27, batch 120, loss=3.1953\n",
      "  Epoch 27, batch 140, loss=3.1910\n",
      "  Epoch 27, batch 160, loss=3.2191\n",
      "  Epoch 27, batch 180, loss=3.2897\n",
      "Epoch [27/50] - Contrastive loss: 3.2112\n",
      "Starting epoch 28/50\n",
      "  Epoch 28, batch 20, loss=3.1928\n",
      "  Epoch 28, batch 40, loss=3.2285\n",
      "  Epoch 28, batch 60, loss=3.2051\n",
      "  Epoch 28, batch 80, loss=3.1987\n",
      "  Epoch 28, batch 100, loss=3.2376\n",
      "  Epoch 28, batch 120, loss=3.2452\n",
      "  Epoch 28, batch 140, loss=3.1873\n",
      "  Epoch 28, batch 160, loss=3.1661\n",
      "  Epoch 28, batch 180, loss=3.2139\n",
      "Epoch [28/50] - Contrastive loss: 3.2020\n",
      "Starting epoch 29/50\n",
      "  Epoch 29, batch 20, loss=3.2102\n",
      "  Epoch 29, batch 40, loss=3.2116\n",
      "  Epoch 29, batch 60, loss=3.2336\n",
      "  Epoch 29, batch 80, loss=3.2110\n",
      "  Epoch 29, batch 100, loss=3.2059\n",
      "  Epoch 29, batch 120, loss=3.1207\n",
      "  Epoch 29, batch 140, loss=3.2448\n",
      "  Epoch 29, batch 160, loss=3.1497\n",
      "  Epoch 29, batch 180, loss=3.2077\n",
      "Epoch [29/50] - Contrastive loss: 3.2004\n",
      "Starting epoch 30/50\n",
      "  Epoch 30, batch 20, loss=3.2296\n",
      "  Epoch 30, batch 40, loss=3.1339\n",
      "  Epoch 30, batch 60, loss=3.2255\n",
      "  Epoch 30, batch 80, loss=3.2511\n",
      "  Epoch 30, batch 100, loss=3.2149\n",
      "  Epoch 30, batch 120, loss=3.1684\n",
      "  Epoch 30, batch 140, loss=3.1701\n",
      "  Epoch 30, batch 160, loss=3.1710\n",
      "  Epoch 30, batch 180, loss=3.1941\n",
      "Epoch [30/50] - Contrastive loss: 3.2039\n",
      "Starting epoch 31/50\n",
      "  Epoch 31, batch 20, loss=3.1796\n",
      "  Epoch 31, batch 40, loss=3.1771\n",
      "  Epoch 31, batch 60, loss=3.2220\n",
      "  Epoch 31, batch 80, loss=3.1725\n",
      "  Epoch 31, batch 100, loss=3.1866\n",
      "  Epoch 31, batch 120, loss=3.1637\n",
      "  Epoch 31, batch 140, loss=3.1854\n",
      "  Epoch 31, batch 160, loss=3.1993\n",
      "  Epoch 31, batch 180, loss=3.1906\n",
      "Epoch [31/50] - Contrastive loss: 3.1981\n",
      "Starting epoch 32/50\n",
      "  Epoch 32, batch 20, loss=3.1635\n",
      "  Epoch 32, batch 40, loss=3.1433\n",
      "  Epoch 32, batch 60, loss=3.2152\n",
      "  Epoch 32, batch 80, loss=3.2179\n",
      "  Epoch 32, batch 100, loss=3.2085\n",
      "  Epoch 32, batch 120, loss=3.1849\n",
      "  Epoch 32, batch 140, loss=3.1938\n",
      "  Epoch 32, batch 160, loss=3.1804\n",
      "  Epoch 32, batch 180, loss=3.1869\n",
      "Epoch [32/50] - Contrastive loss: 3.1882\n",
      "Starting epoch 33/50\n",
      "  Epoch 33, batch 20, loss=3.1700\n",
      "  Epoch 33, batch 40, loss=3.1959\n",
      "  Epoch 33, batch 60, loss=3.1973\n",
      "  Epoch 33, batch 80, loss=3.2235\n",
      "  Epoch 33, batch 100, loss=3.1804\n",
      "  Epoch 33, batch 120, loss=3.2027\n",
      "  Epoch 33, batch 140, loss=3.1794\n",
      "  Epoch 33, batch 160, loss=3.2249\n",
      "  Epoch 33, batch 180, loss=3.2239\n",
      "Epoch [33/50] - Contrastive loss: 3.1899\n",
      "Starting epoch 34/50\n",
      "  Epoch 34, batch 20, loss=3.1762\n",
      "  Epoch 34, batch 40, loss=3.1322\n",
      "  Epoch 34, batch 60, loss=3.2027\n",
      "  Epoch 34, batch 80, loss=3.1991\n",
      "  Epoch 34, batch 100, loss=3.1810\n",
      "  Epoch 34, batch 120, loss=3.1904\n",
      "  Epoch 34, batch 140, loss=3.1380\n",
      "  Epoch 34, batch 160, loss=3.1889\n",
      "  Epoch 34, batch 180, loss=3.1752\n",
      "Epoch [34/50] - Contrastive loss: 3.1897\n",
      "Starting epoch 35/50\n",
      "  Epoch 35, batch 20, loss=3.2203\n",
      "  Epoch 35, batch 40, loss=3.1529\n",
      "  Epoch 35, batch 60, loss=3.2160\n",
      "  Epoch 35, batch 80, loss=3.1679\n",
      "  Epoch 35, batch 100, loss=3.1998\n",
      "  Epoch 35, batch 120, loss=3.2007\n",
      "  Epoch 35, batch 140, loss=3.1346\n",
      "  Epoch 35, batch 160, loss=3.1669\n",
      "  Epoch 35, batch 180, loss=3.1861\n",
      "Epoch [35/50] - Contrastive loss: 3.1822\n",
      "Starting epoch 36/50\n",
      "  Epoch 36, batch 20, loss=3.1694\n",
      "  Epoch 36, batch 40, loss=3.1498\n",
      "  Epoch 36, batch 60, loss=3.2342\n",
      "  Epoch 36, batch 80, loss=3.1869\n",
      "  Epoch 36, batch 100, loss=3.1952\n",
      "  Epoch 36, batch 120, loss=3.1670\n",
      "  Epoch 36, batch 140, loss=3.1602\n",
      "  Epoch 36, batch 160, loss=3.1832\n",
      "  Epoch 36, batch 180, loss=3.1800\n",
      "Epoch [36/50] - Contrastive loss: 3.1885\n",
      "Starting epoch 37/50\n",
      "  Epoch 37, batch 20, loss=3.1794\n",
      "  Epoch 37, batch 40, loss=3.1441\n",
      "  Epoch 37, batch 60, loss=3.1402\n",
      "  Epoch 37, batch 80, loss=3.1635\n",
      "  Epoch 37, batch 100, loss=3.1955\n",
      "  Epoch 37, batch 120, loss=3.1902\n",
      "  Epoch 37, batch 140, loss=3.1789\n",
      "  Epoch 37, batch 160, loss=3.1591\n",
      "  Epoch 37, batch 180, loss=3.1718\n",
      "Epoch [37/50] - Contrastive loss: 3.1751\n",
      "Starting epoch 38/50\n",
      "  Epoch 38, batch 20, loss=3.1795\n",
      "  Epoch 38, batch 40, loss=3.1475\n",
      "  Epoch 38, batch 60, loss=3.1713\n",
      "  Epoch 38, batch 80, loss=3.1816\n",
      "  Epoch 38, batch 100, loss=3.2410\n",
      "  Epoch 38, batch 120, loss=3.2030\n",
      "  Epoch 38, batch 140, loss=3.1803\n",
      "  Epoch 38, batch 160, loss=3.1980\n",
      "  Epoch 38, batch 180, loss=3.2057\n",
      "Epoch [38/50] - Contrastive loss: 3.1780\n",
      "Starting epoch 39/50\n",
      "  Epoch 39, batch 20, loss=3.1246\n",
      "  Epoch 39, batch 40, loss=3.1697\n",
      "  Epoch 39, batch 60, loss=3.1545\n",
      "  Epoch 39, batch 80, loss=3.1501\n",
      "  Epoch 39, batch 100, loss=3.1771\n",
      "  Epoch 39, batch 120, loss=3.1865\n",
      "  Epoch 39, batch 140, loss=3.1506\n",
      "  Epoch 39, batch 160, loss=3.1971\n",
      "  Epoch 39, batch 180, loss=3.1436\n",
      "Epoch [39/50] - Contrastive loss: 3.1772\n",
      "Starting epoch 40/50\n",
      "  Epoch 40, batch 20, loss=3.2343\n",
      "  Epoch 40, batch 40, loss=3.1243\n",
      "  Epoch 40, batch 60, loss=3.1648\n",
      "  Epoch 40, batch 80, loss=3.1408\n",
      "  Epoch 40, batch 100, loss=3.1577\n",
      "  Epoch 40, batch 120, loss=3.1745\n",
      "  Epoch 40, batch 140, loss=3.1270\n",
      "  Epoch 40, batch 160, loss=3.1916\n",
      "  Epoch 40, batch 180, loss=3.1198\n",
      "Epoch [40/50] - Contrastive loss: 3.1676\n",
      "Starting epoch 41/50\n",
      "  Epoch 41, batch 20, loss=3.2076\n",
      "  Epoch 41, batch 40, loss=3.1878\n",
      "  Epoch 41, batch 60, loss=3.1792\n",
      "  Epoch 41, batch 80, loss=3.1461\n",
      "  Epoch 41, batch 100, loss=3.1602\n",
      "  Epoch 41, batch 120, loss=3.1541\n",
      "  Epoch 41, batch 140, loss=3.1423\n",
      "  Epoch 41, batch 160, loss=3.1630\n",
      "  Epoch 41, batch 180, loss=3.1699\n",
      "Epoch [41/50] - Contrastive loss: 3.1692\n",
      "Starting epoch 42/50\n",
      "  Epoch 42, batch 20, loss=3.2319\n",
      "  Epoch 42, batch 40, loss=3.1589\n",
      "  Epoch 42, batch 60, loss=3.1816\n",
      "  Epoch 42, batch 80, loss=3.1788\n",
      "  Epoch 42, batch 100, loss=3.1864\n",
      "  Epoch 42, batch 120, loss=3.1678\n",
      "  Epoch 42, batch 140, loss=3.1632\n",
      "  Epoch 42, batch 160, loss=3.1522\n",
      "  Epoch 42, batch 180, loss=3.1356\n",
      "Epoch [42/50] - Contrastive loss: 3.1727\n",
      "Starting epoch 43/50\n",
      "  Epoch 43, batch 20, loss=3.1609\n",
      "  Epoch 43, batch 40, loss=3.1636\n",
      "  Epoch 43, batch 60, loss=3.1863\n",
      "  Epoch 43, batch 80, loss=3.1766\n",
      "  Epoch 43, batch 100, loss=3.1753\n",
      "  Epoch 43, batch 120, loss=3.1595\n",
      "  Epoch 43, batch 140, loss=3.1409\n",
      "  Epoch 43, batch 160, loss=3.1350\n",
      "  Epoch 43, batch 180, loss=3.1572\n",
      "Epoch [43/50] - Contrastive loss: 3.1654\n",
      "Starting epoch 44/50\n",
      "  Epoch 44, batch 20, loss=3.1712\n",
      "  Epoch 44, batch 40, loss=3.1775\n",
      "  Epoch 44, batch 60, loss=3.1361\n",
      "  Epoch 44, batch 80, loss=3.1377\n",
      "  Epoch 44, batch 100, loss=3.1215\n",
      "  Epoch 44, batch 120, loss=3.1725\n",
      "  Epoch 44, batch 140, loss=3.1472\n",
      "  Epoch 44, batch 160, loss=3.1321\n",
      "  Epoch 44, batch 180, loss=3.1721\n",
      "Epoch [44/50] - Contrastive loss: 3.1625\n",
      "Starting epoch 45/50\n",
      "  Epoch 45, batch 20, loss=3.1330\n",
      "  Epoch 45, batch 40, loss=3.1528\n",
      "  Epoch 45, batch 60, loss=3.1806\n",
      "  Epoch 45, batch 80, loss=3.1826\n",
      "  Epoch 45, batch 100, loss=3.1455\n",
      "  Epoch 45, batch 120, loss=3.1600\n",
      "  Epoch 45, batch 140, loss=3.1643\n",
      "  Epoch 45, batch 160, loss=3.1609\n",
      "  Epoch 45, batch 180, loss=3.1629\n",
      "Epoch [45/50] - Contrastive loss: 3.1635\n",
      "Starting epoch 46/50\n",
      "  Epoch 46, batch 20, loss=3.1766\n",
      "  Epoch 46, batch 40, loss=3.2122\n",
      "  Epoch 46, batch 60, loss=3.1627\n",
      "  Epoch 46, batch 80, loss=3.1563\n",
      "  Epoch 46, batch 100, loss=3.1626\n",
      "  Epoch 46, batch 120, loss=3.1486\n",
      "  Epoch 46, batch 140, loss=3.1321\n",
      "  Epoch 46, batch 160, loss=3.1761\n",
      "  Epoch 46, batch 180, loss=3.1746\n",
      "Epoch [46/50] - Contrastive loss: 3.1565\n",
      "Starting epoch 47/50\n",
      "  Epoch 47, batch 20, loss=3.1703\n",
      "  Epoch 47, batch 40, loss=3.1418\n",
      "  Epoch 47, batch 60, loss=3.2060\n",
      "  Epoch 47, batch 80, loss=3.1655\n",
      "  Epoch 47, batch 100, loss=3.1323\n",
      "  Epoch 47, batch 120, loss=3.1621\n",
      "  Epoch 47, batch 140, loss=3.1786\n",
      "  Epoch 47, batch 160, loss=3.1259\n",
      "  Epoch 47, batch 180, loss=3.1599\n",
      "Epoch [47/50] - Contrastive loss: 3.1527\n",
      "Starting epoch 48/50\n",
      "  Epoch 48, batch 20, loss=3.1396\n",
      "  Epoch 48, batch 40, loss=3.1840\n",
      "  Epoch 48, batch 60, loss=3.1662\n",
      "  Epoch 48, batch 80, loss=3.1744\n",
      "  Epoch 48, batch 100, loss=3.1543\n",
      "  Epoch 48, batch 120, loss=3.1031\n",
      "  Epoch 48, batch 140, loss=3.1740\n",
      "  Epoch 48, batch 160, loss=3.1197\n",
      "  Epoch 48, batch 180, loss=3.1604\n",
      "Epoch [48/50] - Contrastive loss: 3.1530\n",
      "Starting epoch 49/50\n",
      "  Epoch 49, batch 20, loss=3.1599\n",
      "  Epoch 49, batch 40, loss=3.1449\n",
      "  Epoch 49, batch 60, loss=3.1469\n",
      "  Epoch 49, batch 80, loss=3.1417\n",
      "  Epoch 49, batch 100, loss=3.1669\n",
      "  Epoch 49, batch 120, loss=3.1865\n",
      "  Epoch 49, batch 140, loss=3.1640\n",
      "  Epoch 49, batch 160, loss=3.1202\n",
      "  Epoch 49, batch 180, loss=3.1746\n",
      "Epoch [49/50] - Contrastive loss: 3.1545\n",
      "Starting epoch 50/50\n",
      "  Epoch 50, batch 20, loss=3.1684\n",
      "  Epoch 50, batch 40, loss=3.1423\n",
      "  Epoch 50, batch 60, loss=3.1307\n",
      "  Epoch 50, batch 80, loss=3.1620\n",
      "  Epoch 50, batch 100, loss=3.1287\n",
      "  Epoch 50, batch 120, loss=3.1605\n",
      "  Epoch 50, batch 140, loss=3.1249\n",
      "  Epoch 50, batch 160, loss=3.1134\n",
      "  Epoch 50, batch 180, loss=3.1718\n",
      "Epoch [50/50] - Contrastive loss: 3.1498\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    feature_dim = 256\n",
    "    projection_dim = 128\n",
    "    temperature = 0.5\n",
    "    epochs = 50\n",
    "\n",
    "    model = ContrastiveModel(\n",
    "        feature_dim=feature_dim,\n",
    "        projection_dim=projection_dim\n",
    "    ).to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(\n",
    "        model.parameters(),\n",
    "        lr=1e-3,\n",
    "        weight_decay=1e-4\n",
    "    )\n",
    "\n",
    "    train_contrastive(\n",
    "        model,\n",
    "        contrastive_loader,\n",
    "        optimizer,\n",
    "        device,\n",
    "        epochs=epochs,\n",
    "        temperature=temperature\n",
    "    )\n",
    "\n",
    "    torch.save(model.encoder.state_dict(), \"footprint_encoder_contrastive.pth\")\n",
    "\n",
    "\n",
    "main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
